{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-8K6X-tXJbPx"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "u_AtYe4RXxUM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Implementing A Simple Transformer with Attention"
      ],
      "metadata": {
        "id": "E31ZRLdtGJlQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## courtesy\n",
        "\n",
        "https://arxiv.org/abs/1706.03762\n",
        "\n",
        "Attention Is All You Need\n",
        "\n",
        "Different GPT blogs"
      ],
      "metadata": {
        "id": "ylmAAmzUF539"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Install libraries"
      ],
      "metadata": {
        "id": "P--170vF-bNM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch==2.0.1 torchtext==0.15.2  torchvision"
      ],
      "metadata": {
        "id": "unKPJjMWY4xU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c7df5706-fa8b-4e93-d7e1-189186b1811e"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torch==2.0.1\n",
            "  Downloading torch-2.0.1-cp310-cp310-manylinux1_x86_64.whl.metadata (24 kB)\n",
            "Collecting torchtext==0.15.2\n",
            "  Downloading torchtext-0.15.2-cp310-cp310-manylinux1_x86_64.whl.metadata (7.4 kB)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.20.1+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1) (3.1.4)\n",
            "Collecting nvidia-cuda-nvrtc-cu11==11.7.99 (from torch==2.0.1)\n",
            "  Downloading nvidia_cuda_nvrtc_cu11-11.7.99-2-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu11==11.7.99 (from torch==2.0.1)\n",
            "  Downloading nvidia_cuda_runtime_cu11-11.7.99-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cuda-cupti-cu11==11.7.101 (from torch==2.0.1)\n",
            "  Downloading nvidia_cuda_cupti_cu11-11.7.101-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu11==8.5.0.96 (from torch==2.0.1)\n",
            "  Downloading nvidia_cudnn_cu11-8.5.0.96-2-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu11==11.10.3.66 (from torch==2.0.1)\n",
            "  Downloading nvidia_cublas_cu11-11.10.3.66-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cufft-cu11==10.9.0.58 (from torch==2.0.1)\n",
            "  Downloading nvidia_cufft_cu11-10.9.0.58-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu11==10.2.10.91 (from torch==2.0.1)\n",
            "  Downloading nvidia_curand_cu11-10.2.10.91-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusolver-cu11==11.4.0.1 (from torch==2.0.1)\n",
            "  Downloading nvidia_cusolver_cu11-11.4.0.1-2-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu11==11.7.4.91 (from torch==2.0.1)\n",
            "  Downloading nvidia_cusparse_cu11-11.7.4.91-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-nccl-cu11==2.14.3 (from torch==2.0.1)\n",
            "  Downloading nvidia_nccl_cu11-2.14.3-py3-none-manylinux1_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-nvtx-cu11==11.7.91 (from torch==2.0.1)\n",
            "  Downloading nvidia_nvtx_cu11-11.7.91-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting triton==2.0.0 (from torch==2.0.1)\n",
            "  Downloading triton-2.0.0-1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.0 kB)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torchtext==0.15.2) (4.67.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchtext==0.15.2) (2.32.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchtext==0.15.2) (1.26.4)\n",
            "Collecting torchdata==0.6.1 (from torchtext==0.15.2)\n",
            "  Downloading torchdata-0.6.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch==2.0.1) (75.1.0)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.10/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch==2.0.1) (0.45.1)\n",
            "Requirement already satisfied: urllib3>=1.25 in /usr/local/lib/python3.10/dist-packages (from torchdata==0.6.1->torchtext==0.15.2) (2.2.3)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch==2.0.1) (3.31.2)\n",
            "Collecting lit (from triton==2.0.0->torch==2.0.1)\n",
            "  Downloading lit-18.1.8-py3-none-any.whl.metadata (2.5 kB)\n",
            "INFO: pip is looking at multiple versions of torchvision to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting torchvision\n",
            "  Downloading torchvision-0.20.1-cp310-cp310-manylinux1_x86_64.whl.metadata (6.1 kB)\n",
            "  Downloading torchvision-0.20.0-cp310-cp310-manylinux1_x86_64.whl.metadata (6.1 kB)\n",
            "  Downloading torchvision-0.19.1-cp310-cp310-manylinux1_x86_64.whl.metadata (6.0 kB)\n",
            "  Downloading torchvision-0.19.0-cp310-cp310-manylinux1_x86_64.whl.metadata (6.0 kB)\n",
            "  Downloading torchvision-0.18.1-cp310-cp310-manylinux1_x86_64.whl.metadata (6.6 kB)\n",
            "  Downloading torchvision-0.18.0-cp310-cp310-manylinux1_x86_64.whl.metadata (6.6 kB)\n",
            "  Downloading torchvision-0.17.2-cp310-cp310-manylinux1_x86_64.whl.metadata (6.6 kB)\n",
            "INFO: pip is still looking at multiple versions of torchvision to determine which version is compatible with other requirements. This could take a while.\n",
            "  Downloading torchvision-0.17.1-cp310-cp310-manylinux1_x86_64.whl.metadata (6.6 kB)\n",
            "  Downloading torchvision-0.17.0-cp310-cp310-manylinux1_x86_64.whl.metadata (6.6 kB)\n",
            "  Downloading torchvision-0.16.2-cp310-cp310-manylinux1_x86_64.whl.metadata (6.6 kB)\n",
            "  Downloading torchvision-0.16.1-cp310-cp310-manylinux1_x86_64.whl.metadata (6.6 kB)\n",
            "  Downloading torchvision-0.16.0-cp310-cp310-manylinux1_x86_64.whl.metadata (6.6 kB)\n",
            "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
            "  Downloading torchvision-0.15.2-cp310-cp310-manylinux1_x86_64.whl.metadata (11 kB)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (11.0.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.0.1) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext==0.15.2) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext==0.15.2) (3.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext==0.15.2) (2024.12.14)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch==2.0.1) (1.3.0)\n",
            "Downloading torch-2.0.1-cp310-cp310-manylinux1_x86_64.whl (619.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m619.9/619.9 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torchtext-0.15.2-cp310-cp310-manylinux1_x86_64.whl (2.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m49.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu11-11.10.3.66-py3-none-manylinux1_x86_64.whl (317.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.1/317.1 MB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu11-11.7.101-py3-none-manylinux1_x86_64.whl (11.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.8/11.8 MB\u001b[0m \u001b[31m28.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu11-11.7.99-2-py3-none-manylinux1_x86_64.whl (21.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.0/21.0 MB\u001b[0m \u001b[31m32.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu11-11.7.99-py3-none-manylinux1_x86_64.whl (849 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m849.3/849.3 kB\u001b[0m \u001b[31m41.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu11-8.5.0.96-2-py3-none-manylinux1_x86_64.whl (557.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m557.1/557.1 MB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu11-10.9.0.58-py3-none-manylinux2014_x86_64.whl (168.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m168.4/168.4 MB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu11-10.2.10.91-py3-none-manylinux1_x86_64.whl (54.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.6/54.6 MB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu11-11.4.0.1-2-py3-none-manylinux1_x86_64.whl (102.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m102.6/102.6 MB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu11-11.7.4.91-py3-none-manylinux1_x86_64.whl (173.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m173.2/173.2 MB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nccl_cu11-2.14.3-py3-none-manylinux1_x86_64.whl (177.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m177.1/177.1 MB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvtx_cu11-11.7.91-py3-none-manylinux1_x86_64.whl (98 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.6/98.6 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torchdata-0.6.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.6/4.6 MB\u001b[0m \u001b[31m26.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading triton-2.0.0-1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (63.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.3/63.3 MB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torchvision-0.15.2-cp310-cp310-manylinux1_x86_64.whl (6.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.0/6.0 MB\u001b[0m \u001b[31m70.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading lit-18.1.8-py3-none-any.whl (96 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m96.4/96.4 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: lit, nvidia-nvtx-cu11, nvidia-nccl-cu11, nvidia-cusparse-cu11, nvidia-curand-cu11, nvidia-cufft-cu11, nvidia-cuda-runtime-cu11, nvidia-cuda-nvrtc-cu11, nvidia-cuda-cupti-cu11, nvidia-cublas-cu11, nvidia-cusolver-cu11, nvidia-cudnn-cu11, triton, torch, torchdata, torchvision, torchtext\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.5.1+cu121\n",
            "    Uninstalling torch-2.5.1+cu121:\n",
            "      Successfully uninstalled torch-2.5.1+cu121\n",
            "  Attempting uninstall: torchvision\n",
            "    Found existing installation: torchvision 0.20.1+cu121\n",
            "    Uninstalling torchvision-0.20.1+cu121:\n",
            "      Successfully uninstalled torchvision-0.20.1+cu121\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchaudio 2.5.1+cu121 requires torch==2.5.1, but you have torch 2.0.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed lit-18.1.8 nvidia-cublas-cu11-11.10.3.66 nvidia-cuda-cupti-cu11-11.7.101 nvidia-cuda-nvrtc-cu11-11.7.99 nvidia-cuda-runtime-cu11-11.7.99 nvidia-cudnn-cu11-8.5.0.96 nvidia-cufft-cu11-10.9.0.58 nvidia-curand-cu11-10.2.10.91 nvidia-cusolver-cu11-11.4.0.1 nvidia-cusparse-cu11-11.7.4.91 nvidia-nccl-cu11-2.14.3 nvidia-nvtx-cu11-11.7.91 torch-2.0.1 torchdata-0.6.1 torchtext-0.15.2 torchvision-0.15.2 triton-2.0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 1: Import Libraries & Prepare Dataset\n",
        "\n",
        "\n",
        "We break down sentences into words (tokenization) and convert these words into numerical values (numericalization) so the computer can process them. To ensure that all sentences have the same length, we add padding where necessary.\n",
        "\n",
        "\n",
        "Imagine you have different sentences, but some are long and some are short. The computer likes things to be the same size, so we add extra blank spaces (padding) to make all sentences the same length."
      ],
      "metadata": {
        "id": "jYHHk4ty-zTD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Import Libraries & Prepare Dataset\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import os\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Define dataset with diverse sentences\n",
        "text = [\n",
        "    \"deep learning enables AI to process data efficiently\",\n",
        "    \"attention is the key mechanism behind transformers\",\n",
        "    \"transformers improve machine learning and NLP models\",\n",
        "    \"models generalize better with diverse training datasets\",\n",
        "    \"neural networks learn from both structured and unstructured data\",\n",
        "    \"machine learning optimizes healthcare and financial decisions\",\n",
        "    \"artificial intelligence advancements rely on large datasets\",\n",
        "    \"language models predict sequences for a variety of applications\",\n",
        "    \"transfer learning helps models reuse knowledge across domains\",\n",
        "    \"self-attention allows transformers to process entire sequences in parallel\",\n",
        "]\n",
        "\n",
        "\n",
        "text.extend([\n",
        "    \"transformers use multi-head self-attention to capture dependencies\",\n",
        "    \"machine learning requires extensive training data\",\n",
        "    \"deep neural networks power modern AI models\",\n",
        "    \"natural language processing is a key area of artificial intelligence\",\n",
        "    \"self-supervised learning reduces the need for labeled data\",\n",
        "    \"vision transformers are effective for image recognition tasks\",\n",
        "    \"sequence-to-sequence models generate text translations\",\n",
        "    \"speech recognition systems use recurrent networks\",\n",
        "    \"deep learning applications span across multiple industries\",\n",
        "    \"transfer learning helps models adapt to new tasks efficiently\",\n",
        "    \"reinforcement learning is used in robotics and game AI\",\n",
        "    \"contrastive learning improves representations in self-supervised learning\",\n",
        "    \"large language models like GPT understand and generate text\",\n",
        "    \"computer vision detects objects, faces, and scenes in images\",\n",
        "    \"attention mechanisms help models focus on relevant information\",\n",
        "    \"transformers surpass recurrent networks in processing sequences\",\n",
        "    \"neural networks learn hierarchical features from raw data\",\n",
        "    \"speech synthesis models generate human-like voices\",\n",
        "    \"convolutional neural networks excel in image processing\",\n",
        "    \"language models predict the next word in a sentence\",\n",
        "    \"unsupervised learning finds hidden patterns in unlabeled data\",\n",
        "    \"generative models create realistic images and videos\",\n",
        "    \"transformers process entire sequences in parallel\",\n",
        "    \"BERT is a transformer model pre-trained on large text corpora\",\n",
        "    \"autonomous vehicles rely on deep learning for perception\",\n",
        "    \"recurrent neural networks handle sequential data efficiently\",\n",
        "    \"zero-shot learning enables models to generalize to unseen tasks\",\n",
        "    \"few-shot learning trains models with very few examples\",\n",
        "    \"multi-modal models integrate text, images, and audio\",\n",
        "    \"transformers achieve state-of-the-art results in NLP\",\n",
        "    \"self-attention captures long-range dependencies in sequences\",\n",
        "    \"meta-learning allows models to learn how to learn\",\n",
        "    \"language understanding is crucial for conversational AI\",\n",
        "    \"convolutional networks extract spatial features in vision tasks\",\n",
        "    \"transformers are trained using large-scale parallel computing\",\n",
        "    \"speech-to-text models convert spoken words into written text\",\n",
        "    \"recurrent networks struggle with long-term dependencies\",\n",
        "    \"contrastive learning is widely used in representation learning\",\n",
        "    \"neural networks optimize parameters using backpropagation\",\n",
        "    \"few-shot learning mimics human ability to learn from few examples\",\n",
        "    \"transformers enable contextualized embeddings in language models\",\n",
        "    \"deep learning accelerates drug discovery and medical research\",\n",
        "    \"attention is all you need is a seminal paper on transformers\",\n",
        "    \"transformer-based architectures improve translation accuracy\",\n",
        "    \"transformers eliminate the need for recurrence in NLP models\",\n",
        "    \"speech models use hidden Markov models and neural networks\",\n",
        "    \"transformers outperform traditional sequence models in NLP\",\n",
        "    \"pre-training on large datasets enhances model generalization\",\n",
        "    \"tokenization is a fundamental step in NLP preprocessing\",\n",
        "    \"transformers generalize well across different NLP tasks\",\n",
        "    \"AI systems leverage deep learning for better decision-making\",\n",
        "])\n",
        "\n",
        "\n",
        "# Tokenization\n",
        "tokenizer = get_tokenizer(\"basic_english\")\n",
        "tokens = [tokenizer(sentence) for sentence in text]\n",
        "\n",
        "# Debugging: Print tokenized text\n",
        "print(\"Tokenized sentences:\", tokens)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "boCqi8TR-0RH",
        "outputId": "98744aec-c2be-4b06-ddc9-aa97697e2124"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokenized sentences: [['deep', 'learning', 'enables', 'ai', 'to', 'process', 'data', 'efficiently'], ['attention', 'is', 'the', 'key', 'mechanism', 'behind', 'transformers'], ['transformers', 'improve', 'machine', 'learning', 'and', 'nlp', 'models'], ['models', 'generalize', 'better', 'with', 'diverse', 'training', 'datasets'], ['neural', 'networks', 'learn', 'from', 'both', 'structured', 'and', 'unstructured', 'data'], ['machine', 'learning', 'optimizes', 'healthcare', 'and', 'financial', 'decisions'], ['artificial', 'intelligence', 'advancements', 'rely', 'on', 'large', 'datasets'], ['language', 'models', 'predict', 'sequences', 'for', 'a', 'variety', 'of', 'applications'], ['transfer', 'learning', 'helps', 'models', 'reuse', 'knowledge', 'across', 'domains'], ['self-attention', 'allows', 'transformers', 'to', 'process', 'entire', 'sequences', 'in', 'parallel'], ['transformers', 'use', 'multi-head', 'self-attention', 'to', 'capture', 'dependencies'], ['machine', 'learning', 'requires', 'extensive', 'training', 'data'], ['deep', 'neural', 'networks', 'power', 'modern', 'ai', 'models'], ['natural', 'language', 'processing', 'is', 'a', 'key', 'area', 'of', 'artificial', 'intelligence'], ['self-supervised', 'learning', 'reduces', 'the', 'need', 'for', 'labeled', 'data'], ['vision', 'transformers', 'are', 'effective', 'for', 'image', 'recognition', 'tasks'], ['sequence-to-sequence', 'models', 'generate', 'text', 'translations'], ['speech', 'recognition', 'systems', 'use', 'recurrent', 'networks'], ['deep', 'learning', 'applications', 'span', 'across', 'multiple', 'industries'], ['transfer', 'learning', 'helps', 'models', 'adapt', 'to', 'new', 'tasks', 'efficiently'], ['reinforcement', 'learning', 'is', 'used', 'in', 'robotics', 'and', 'game', 'ai'], ['contrastive', 'learning', 'improves', 'representations', 'in', 'self-supervised', 'learning'], ['large', 'language', 'models', 'like', 'gpt', 'understand', 'and', 'generate', 'text'], ['computer', 'vision', 'detects', 'objects', ',', 'faces', ',', 'and', 'scenes', 'in', 'images'], ['attention', 'mechanisms', 'help', 'models', 'focus', 'on', 'relevant', 'information'], ['transformers', 'surpass', 'recurrent', 'networks', 'in', 'processing', 'sequences'], ['neural', 'networks', 'learn', 'hierarchical', 'features', 'from', 'raw', 'data'], ['speech', 'synthesis', 'models', 'generate', 'human-like', 'voices'], ['convolutional', 'neural', 'networks', 'excel', 'in', 'image', 'processing'], ['language', 'models', 'predict', 'the', 'next', 'word', 'in', 'a', 'sentence'], ['unsupervised', 'learning', 'finds', 'hidden', 'patterns', 'in', 'unlabeled', 'data'], ['generative', 'models', 'create', 'realistic', 'images', 'and', 'videos'], ['transformers', 'process', 'entire', 'sequences', 'in', 'parallel'], ['bert', 'is', 'a', 'transformer', 'model', 'pre-trained', 'on', 'large', 'text', 'corpora'], ['autonomous', 'vehicles', 'rely', 'on', 'deep', 'learning', 'for', 'perception'], ['recurrent', 'neural', 'networks', 'handle', 'sequential', 'data', 'efficiently'], ['zero-shot', 'learning', 'enables', 'models', 'to', 'generalize', 'to', 'unseen', 'tasks'], ['few-shot', 'learning', 'trains', 'models', 'with', 'very', 'few', 'examples'], ['multi-modal', 'models', 'integrate', 'text', ',', 'images', ',', 'and', 'audio'], ['transformers', 'achieve', 'state-of-the-art', 'results', 'in', 'nlp'], ['self-attention', 'captures', 'long-range', 'dependencies', 'in', 'sequences'], ['meta-learning', 'allows', 'models', 'to', 'learn', 'how', 'to', 'learn'], ['language', 'understanding', 'is', 'crucial', 'for', 'conversational', 'ai'], ['convolutional', 'networks', 'extract', 'spatial', 'features', 'in', 'vision', 'tasks'], ['transformers', 'are', 'trained', 'using', 'large-scale', 'parallel', 'computing'], ['speech-to-text', 'models', 'convert', 'spoken', 'words', 'into', 'written', 'text'], ['recurrent', 'networks', 'struggle', 'with', 'long-term', 'dependencies'], ['contrastive', 'learning', 'is', 'widely', 'used', 'in', 'representation', 'learning'], ['neural', 'networks', 'optimize', 'parameters', 'using', 'backpropagation'], ['few-shot', 'learning', 'mimics', 'human', 'ability', 'to', 'learn', 'from', 'few', 'examples'], ['transformers', 'enable', 'contextualized', 'embeddings', 'in', 'language', 'models'], ['deep', 'learning', 'accelerates', 'drug', 'discovery', 'and', 'medical', 'research'], ['attention', 'is', 'all', 'you', 'need', 'is', 'a', 'seminal', 'paper', 'on', 'transformers'], ['transformer-based', 'architectures', 'improve', 'translation', 'accuracy'], ['transformers', 'eliminate', 'the', 'need', 'for', 'recurrence', 'in', 'nlp', 'models'], ['speech', 'models', 'use', 'hidden', 'markov', 'models', 'and', 'neural', 'networks'], ['transformers', 'outperform', 'traditional', 'sequence', 'models', 'in', 'nlp'], ['pre-training', 'on', 'large', 'datasets', 'enhances', 'model', 'generalization'], ['tokenization', 'is', 'a', 'fundamental', 'step', 'in', 'nlp', 'preprocessing'], ['transformers', 'generalize', 'well', 'across', 'different', 'nlp', 'tasks'], ['ai', 'systems', 'leverage', 'deep', 'learning', 'for', 'better', 'decision-making']]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 1.1 Vocabulary Building\n",
        "\n",
        "\n",
        "A vocabulary is like a dictionary where each word is assigned a unique number. Special tokens like <pad> (for padding) and <unk> (for unknown words) help handle cases where a word isn't recognized.\n",
        "\n",
        "\n",
        "Think of it like a secret codebook where every word gets its own number. If the computer doesn’t know a word, it uses a special code for \"unknown\" words."
      ],
      "metadata": {
        "id": "LTBeF87m_KXX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Build Vocabulary\n",
        "vocab = build_vocab_from_iterator(tokens, specials=[\"<pad>\", \"<unk>\"], min_freq=1)\n",
        "vocab.set_default_index(vocab[\"<unk>\"])\n",
        "\n",
        "# Debugging: Print vocab size\n",
        "print(f\"Vocabulary Size: {len(vocab)}\")\n",
        "\n",
        "# Convert text to numerical format\n",
        "numerical_text = [[vocab[token] for token in sentence] for sentence in tokens]\n",
        "\n",
        "# Debugging: Print numericalized sentences\n",
        "print(\"Numericalized sentences:\", numerical_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NNJFzhZM_RMZ",
        "outputId": "8ab5ce3b-c185-461a-9cf3-0c19c17f51aa"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary Size: 225\n",
            "Numericalized sentences: [[14, 3, 53, 18, 9, 39, 10, 31], [28, 8, 26, 64, 146, 88, 5], [5, 62, 36, 3, 7, 16, 2], [2, 33, 50, 45, 106, 72, 29], [12, 6, 19, 32, 90, 195, 7, 211, 10], [36, 3, 160, 127, 7, 118, 102], [49, 63, 81, 69, 17, 24, 29], [15, 2, 67, 20, 11, 13, 213, 66, 47], [73, 3, 59, 2, 181, 138, 27, 107], [41, 46, 5, 9, 39, 54, 20, 4, 38], [5, 43, 152, 41, 9, 91, 30], [36, 3, 178, 115, 72, 10], [14, 12, 6, 166, 151, 18, 2], [155, 15, 40, 8, 13, 64, 84, 66, 49, 63], [70, 3, 173, 26, 37, 11, 139, 10], [44, 5, 48, 109, 11, 61, 68, 21], [187, 2, 34, 22, 206], [42, 68, 71, 43, 25, 6], [14, 3, 47, 189, 27, 154, 134], [73, 3, 59, 2, 80, 9, 156, 21, 31], [174, 3, 8, 74, 4, 182, 7, 122, 18], [51, 3, 133, 177, 4, 70, 3], [24, 15, 2, 142, 125, 207, 7, 34, 22], [93, 44, 103, 158, 23, 117, 23, 7, 183, 4, 35], [28, 147, 128, 2, 120, 17, 175, 135], [5, 197, 25, 6, 4, 40, 20], [12, 6, 19, 129, 56, 32, 170, 10], [42, 198, 2, 34, 132, 217], [52, 12, 6, 114, 4, 61, 40], [15, 2, 67, 26, 157, 220, 4, 13, 185], [212, 3, 119, 60, 164, 4, 209, 10], [124, 2, 99, 171, 35, 7, 216], [5, 39, 54, 20, 4, 38], [89, 8, 13, 203, 65, 167, 17, 24, 22, 98], [86, 214, 69, 17, 14, 3, 11, 165], [25, 12, 6, 126, 188, 10, 31], [224, 3, 53, 2, 9, 33, 9, 210, 21], [58, 3, 202, 2, 45, 215, 57, 55], [153, 2, 136, 22, 23, 35, 23, 7, 85], [5, 79, 193, 180, 4, 16], [41, 92, 143, 30, 4, 20], [149, 46, 2, 9, 19, 130, 9, 19], [15, 208, 8, 100, 11, 96, 18], [52, 6, 116, 190, 56, 4, 44, 21], [5, 48, 201, 75, 140, 38, 94], [191, 2, 97, 192, 221, 137, 222, 22], [25, 6, 196, 45, 144, 30], [51, 3, 8, 219, 74, 4, 176, 3], [12, 6, 159, 163, 75, 87], [58, 3, 150, 131, 76, 9, 19, 32, 57, 55], [5, 112, 95, 111, 4, 15, 2], [14, 3, 77, 108, 105, 7, 148, 179], [28, 8, 82, 223, 37, 8, 13, 184, 162, 17, 5], [204, 83, 62, 205, 78], [5, 110, 26, 37, 11, 172, 4, 16, 2], [42, 2, 43, 60, 145, 2, 7, 12, 6], [5, 161, 200, 186, 2, 4, 16], [168, 17, 24, 29, 113, 65, 123], [199, 8, 13, 121, 194, 4, 16, 169], [5, 33, 218, 27, 104, 16, 21], [18, 71, 141, 14, 3, 11, 50, 101]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 1.2  Padding\n",
        "\n",
        "Since sentences have different lengths, we use padding tokens (<pad>) to make them uniform in size. This helps in batch processing, making computations more efficient.\n",
        "\n",
        "\n",
        "It’s like writing a sentence on a blank page and adding extra spaces at the end so that all pages have the same number of words. This way, everything lines up properly."
      ],
      "metadata": {
        "id": "_AaYypB-_UmF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Pad sequences\n",
        "max_seq_length = max(len(seq) for seq in numerical_text)\n",
        "padded_text = [F.pad(torch.tensor(seq, dtype=torch.long), (0, max_seq_length - len(seq)), value=vocab[\"<pad>\"]) for seq in numerical_text]\n",
        "\n",
        "# Convert to TensorDataset\n",
        "padded_text = torch.stack(padded_text)\n",
        "dataset = TensorDataset(padded_text)\n",
        "\n",
        "# Create DataLoader\n",
        "batch_size = 4\n",
        "train_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)"
      ],
      "metadata": {
        "id": "lJsBnY4J_WuH"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 2: Define Multi-Head Attention\n",
        "\n",
        "Multi-head attention allows a model to focus on different parts of a sentence at the same time. Instead of just looking at one word at a time, it splits the information into smaller pieces (heads), each attending to different relationships within the text. This helps capture meaning better.\n",
        "\n",
        "For example, in the sentence \"The cat sat on the mat\", different heads might focus on:\n",
        "\n",
        "The relationship between cat and sat,\n",
        "The connection between mat and on,\n",
        "The subject (cat) of the sentence.\n",
        "Each head processes information differently, and the results are combined for better understanding.\n",
        "\n",
        "\n",
        "Imagine you’re watching a movie with your friends. One friend pays attention to the music, another to the colors, another to the action scenes. When you talk about it later, you all bring different details, making the story richer. That’s what multi-head attention does—it looks at different parts of the sentence at the same time."
      ],
      "metadata": {
        "id": "Ld7lvaiN_ep9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Detailed Explanation\n",
        "Multi-head attention is a mechanism that allows a model to focus on different parts of a sentence simultaneously. Instead of analyzing one word at a time, it breaks the information into multiple smaller parts (called \"heads\"), with each head attending to different relationships between words. This helps capture the full meaning of the sentence more effectively.\n",
        "\n",
        "For example, consider the sentence:\n",
        "📝 \"The cat sat on the mat.\"\n",
        "\n",
        "A single-head attention model might focus only on a single aspect of the sentence, such as:\n",
        "\n",
        "The connection between cat and sat (i.e., who is performing the action?).\n",
        "However, a multi-head attention model splits its focus into multiple heads, where:\n",
        "\n",
        "One head looks at the relationship between cat and sat (who is performing the action?).\n",
        "Another head looks at the relationship between mat and on (where did the action take place?).\n",
        "A third head looks at the (determiner) and how it relates to cat (subject).\n",
        "Each head processes different relationships separately, and their results are combined to create a stronger understanding of the sentence.\n",
        "\n",
        "### How Multi-Head Attention Works (Step by Step)\n",
        "Each word in a sentence is transformed into a numerical representation called an embedding. Multi-head attention processes these embeddings using the following steps:\n",
        "\n",
        "#### 1.Linear Transformations:\n",
        "\n",
        "Each word embedding is passed through three linear layers (fully connected layers) to generate:\n",
        "Q (Query): What this word is looking for in other words.\n",
        "K (Key): How relevant this word is to other words.\n",
        "V (Value): The actual information stored in the word.\n",
        "These transformations allow each word to have different perspectives when interacting with other words.\n",
        "\n",
        "\n",
        "\n",
        "##### Understanding Query (Q), Key (K), and Value (V)\n",
        "\n",
        "In the sentence **\"The cat sat on the mat\"**, we use **Query (Q)**, **Key (K)**, and **Value (V)** to determine which words should focus on each other.\n",
        "\n",
        "###### 1. Query (Q) → The word we are focusing on.\n",
        "   - **Example**: `\"cat\"`\n",
        "\n",
        "###### 2. Key (K) → The words we compare with.\n",
        "   - **Example**: `[\"The\", \"cat\", \"sat\", \"on\", \"the\", \"mat\"]`\n",
        "\n",
        "###### 3. Value (V) → The actual word representations used in the output.\n",
        "   - **Example**: `[\"The\", \"cat\", \"sat\", \"on\", \"the\", \"mat\"]`\n",
        "\n",
        "\n",
        "Step-by-Step Breakdown\n",
        "Let's say we want to find what \"cat\" should pay attention to.\n",
        "\n",
        "We compute attention scores by comparing Q (\"cat\") with each K (all words).\n",
        "\n",
        "If \"sat\" has a high score with \"cat\", then \"sat\" is important in relation to \"cat\".\n",
        "\n",
        "The word representations from V (Values) are then weighted by these attention scores to form a meaningful output.\n",
        "\n",
        "##### Example of Attention Scores for \"cat\"\n",
        "\n",
        "| Key Word (K) | Score (Similarity with Q \"cat\") |\n",
        "|-------------|--------------------------------|\n",
        "| \"The\"       | Low (not very related)       |\n",
        "| \"cat\"       | High (itself)                |\n",
        "| \"sat\"       | High (action related to cat) |\n",
        "| \"on\"        | Medium (less relevant)       |\n",
        "| \"the\"       | Low (not important here)     |\n",
        "| \"mat\"       | Medium (cat is on the mat)   |\n",
        "\n",
        "Final Output: \"cat\" will focus most on \"sat\" and \"mat\" because they are relevant.\n",
        "\n",
        "\n",
        "###### Explanation For a 12-Year-Old\n",
        "Think of a classroom:\n",
        "\n",
        "You (Query Q) are \"cat\", and you are curious about something.\n",
        "Your classmates (Keys K) are all the words in the sentence.\n",
        "You ask, \"Who is most important to me?\"\n",
        "Each classmate (word) gives their answer.\n",
        "The teacher (Attention Mechanism) decides which answers (Values V) matter the most based on how similar they are to what you asked.\n",
        "##### Example:\n",
        "\n",
        "\"cat\" will listen most to \"sat\" because cats sit.\n",
        "\"cat\" will also listen to \"mat\", since cats often sit on mats.\n",
        "But \"The\" and \"on\" aren't very important to \"cat\", so it pays less attention to them.\n",
        "##### 🚀 Conclusion: Attention helps words focus on the most meaningful parts of the sentence!\n",
        "\n",
        "#### 2.Splitting into Multiple Heads:\n",
        "\n",
        "The transformed Q, K, and V vectors are split into multiple smaller parts (heads). Each head independently performs attention calculations.\n",
        "\n",
        "#### 3.Scaled Dot-Product Attention:\n",
        "\n",
        "For each head, the model computes attention scores using:\n",
        "$$ Attention \\ Score = \\frac{Q \\times K^T}{\\sqrt{d_k}} $$\n",
        "\n",
        "\n",
        "\n",
        "This means:\n",
        "The query (Q) of one word is compared against the key (K) of every other word to determine how much attention it should give.\n",
        "The dot product measures similarity—higher values mean stronger relationships.\n",
        "The square root of d_k (size of each head) helps stabilize gradients during training.\n",
        "#### 4.Masking (Optional):\n",
        "\n",
        "If using masked attention (common in language models like GPT), we block information from certain positions to prevent the model from \"cheating.\"\n",
        "This is important for autoregressive generation (predicting words one by one) to ensure the model does not see future words.\n",
        "\n",
        "#### 5.Softmax & Weighted Sum:\n",
        "\n",
        "The attention scores are normalized using a softmax function, turning them into probabilities.\n",
        "These probabilities determine how much of each word’s value (V) contributes to the final output.\n",
        "\n",
        "#### 6.Concatenation & Final Transformation:\n",
        "\n",
        "Once each head has computed its result, all heads are concatenated back together and passed through another linear layer to merge their insights.\n",
        "\n",
        "\n",
        "### Simple Explanation For a 12-Year-Old\n",
        "Think of multi-head attention like a group of detectives solving a mystery together.\n",
        "\n",
        "Imagine you're watching a movie with your friends, and each friend focuses on something different:\n",
        "\n",
        "One friend listens to the music 🎵.\n",
        "Another pays attention to the action scenes 🎬.\n",
        "Another watches the characters' emotions 😢.\n",
        "Another looks at the background scenery 🌆.\n",
        "After the movie, when you all discuss it, each friend shares a different part of the story, making your understanding richer and more complete.\n",
        "\n",
        "Similarly, multi-head attention lets a model look at different parts of a sentence at the same time instead of just one word at a time. It understands relationships between words better, just like your friends understand different aspects of the movie!\n",
        "\n",
        "Why is Multi-Head Attention So Powerful?\n",
        "Parallel Processing: Unlike older models (like RNNs), which process words one by one, transformers look at the entire sentence at once.\n",
        "Focus on Different Relationships: Each head pays attention to different parts of the sentence, making the understanding more detailed.\n",
        "Better Context Awareness: Since multiple words can be attended to simultaneously, the model understands long-range dependencies better.\n",
        "This concept is one of the key reasons transformers (like GPT, BERT, and T5) are so powerful in language processing! 🚀"
      ],
      "metadata": {
        "id": "MBIZaVhNETfy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2: Define Multi-Head Attention\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, d_model, num_heads):\n",
        "        super().__init__()\n",
        "        assert d_model % num_heads == 0\n",
        "        self.d_k = d_model // num_heads\n",
        "        self.num_heads = num_heads\n",
        "\n",
        "        self.q_linear = nn.Linear(d_model, d_model)\n",
        "        self.k_linear = nn.Linear(d_model, d_model)\n",
        "        self.v_linear = nn.Linear(d_model, d_model)\n",
        "        self.out_linear = nn.Linear(d_model, d_model)\n",
        "\n",
        "    def forward(self, q, k, v, mask=None):\n",
        "        batch_size = q.size(0)\n",
        "\n",
        "        # Debugging print\n",
        "        #print(f\"Input to attention - Q: {q.shape}, K: {k.shape}, V: {v.shape}\")\n",
        "\n",
        "        q = self.q_linear(q).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
        "        k = self.k_linear(k).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
        "        v = self.v_linear(v).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
        "\n",
        "        # Debugging print\n",
        "        #print(f\"After Linear Layers - Q: {q.shape}, K: {k.shape}, V: {v.shape}\")\n",
        "\n",
        "        scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
        "\n",
        "        if mask is not None:\n",
        "            scores = scores.masked_fill(mask == 0, -1e9)\n",
        "\n",
        "        attn = torch.softmax(scores, dim=-1)\n",
        "\n",
        "        # Debugging print\n",
        "        #print(f\"Attention Scores Shape: {attn.shape}, V Shape: {v.shape}\")\n",
        "\n",
        "        output = torch.matmul(attn, v)\n",
        "\n",
        "        # Debugging print\n",
        "        #print(f\"Output before reshape: {output.shape}\")\n",
        "\n",
        "        output = output.transpose(1, 2).contiguous().view(batch_size, -1, self.num_heads * self.d_k)\n",
        "\n",
        "        #print(f\"Output after reshape: {output.shape}\")\n",
        "\n",
        "        return self.out_linear(output)\n"
      ],
      "metadata": {
        "id": "0aIeqf7t_gAm"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 3. Transformer Model\n",
        "\n",
        "A transformer is a deep learning model that understands text by using layers of multi-head attention and feedforward networks. It doesn’t process words one by one (like RNNs) but instead looks at the whole sentence at once, making it very efficient.\n",
        "\n",
        "How It Works:\n",
        "\n",
        "Embedding Layer: Converts words into numerical vectors that capture their meanings.\n",
        "Positional Encoding: Since transformers don’t read words in order like humans, they add position information to keep track of word order.\n",
        "Multi-Head Attention: Helps the model look at different words in a sentence at once.\n",
        "Feedforward Layers: Refine the information and prepare it for the next layer.\n",
        "Output Layer: Predicts the next word or the meaning of the sentence.\n",
        "Transformers are the backbone of powerful AI models like GPT, BERT, and T5.\n",
        "\n",
        "\n",
        "Think of a transformer like a really smart detective. Instead of reading a book word by word, the detective looks at the entire page, finds important clues, and connects them instantly. It doesn’t just follow the order of words—it figures out how everything fits together to understand the meaning quickly."
      ],
      "metadata": {
        "id": "zJMzkpcd_m7k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Transformer Model\n",
        "class MiniTransformer(nn.Module):\n",
        "    def __init__(self, d_model, num_heads, d_ff, num_layers, vocab_size, max_len):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
        "        self.position_embedding = nn.Embedding(max_len, d_model)\n",
        "        self.layers = nn.ModuleList([MultiHeadAttention(d_model, num_heads) for _ in range(num_layers)])\n",
        "        self.fc_out1 = nn.Linear(d_model, d_model)\n",
        "        self.fc_out2 = nn.Linear(d_model, vocab_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        pos = torch.arange(0, x.size(1)).unsqueeze(0).to(x.device)\n",
        "        x = self.embedding(x) + self.position_embedding(pos)\n",
        "\n",
        "        # Debugging print\n",
        "        #print(f\"Embedding Shape: {x.shape}\")\n",
        "\n",
        "        for layer in self.layers:\n",
        "            x = layer(x, x, x)\n",
        "\n",
        "        x = self.fc_out1(x)\n",
        "        return self.fc_out2(x)\n",
        "\n",
        "# Model Initialization\n",
        "model = MiniTransformer(\n",
        "    d_model=256, num_heads=8, d_ff=512, num_layers=2,\n",
        "    vocab_size=len(vocab), max_len=max_seq_length\n",
        ").to(device)\n",
        "\n",
        "# Debugging: Check embedding layer size\n",
        "print(f\"Embedding Layer Shape: {model.embedding.weight.shape}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZxY74USV_qQF",
        "outputId": "47a0bc13-92ca-40b8-b047-9b1e5870ef22"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Embedding Layer Shape: torch.Size([225, 256])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 4. Train the Model\n",
        "\n",
        "\n",
        "CrossEntropy Loss measures how different the model’s prediction is from the actual answer. Ignoring the <pad> token ensures that padding doesn’t affect training since it's not real data.\n",
        "\n",
        "It’s like grading a test. If a student leaves blank spaces (padding) on the answer sheet, the teacher ignores them while checking answers.\n",
        "\n",
        "\n",
        "AdamW is an optimization algorithm that updates model weights efficiently while preventing overfitting. It’s a version of Adam that includes weight decay, which helps stabilize training.\n",
        "\n",
        "\n",
        "Imagine you’re learning to ride a bike. If you keep adjusting too much, you’ll wobble. If you don’t adjust at all, you’ll fall. AdamW carefully adjusts the learning process so the model doesn’t \"wobble\" too much."
      ],
      "metadata": {
        "id": "fUPsNrzh_wA0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Training\n",
        "def train_model(model, train_loader, epochs=5):\n",
        "    criterion = nn.CrossEntropyLoss(ignore_index=vocab[\"<pad>\"])\n",
        "    #criterion = nn.CrossEntropyLoss(ignore_index=vocab[\"<pad>\"], label_smoothing=0.1)\n",
        "\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=5e-4, weight_decay=1e-4)\n",
        "\n",
        "    model.train()\n",
        "    for epoch in range(epochs):\n",
        "        total_loss = 0\n",
        "        for batch in train_loader:\n",
        "            batch_tensor = batch[0].to(device)\n",
        "            input_tensor = batch_tensor[:, :-1]\n",
        "            target_tensor = batch_tensor[:, 1:]\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            output = model(input_tensor)\n",
        "            loss = criterion(output.reshape(-1, output.size(-1)), target_tensor.reshape(-1))\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        print(f\"Epoch {epoch+1}, Loss: {total_loss:.4f}\")\n",
        "\n",
        "# Train the Model\n",
        "train_model(model, train_loader, epochs=200)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mmhMs6sm_1dr",
        "outputId": "3354019f-b0f4-4638-dc43-34f27b5c831c"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss: 0.8043\n",
            "Epoch 2, Loss: 5.1186\n",
            "Epoch 3, Loss: 9.9844\n",
            "Epoch 4, Loss: 12.6110\n",
            "Epoch 5, Loss: 6.6701\n",
            "Epoch 6, Loss: 6.1210\n",
            "Epoch 7, Loss: 4.2512\n",
            "Epoch 8, Loss: 4.8079\n",
            "Epoch 9, Loss: 4.4147\n",
            "Epoch 10, Loss: 2.8314\n",
            "Epoch 11, Loss: 3.6111\n",
            "Epoch 12, Loss: 5.9703\n",
            "Epoch 13, Loss: 6.3211\n",
            "Epoch 14, Loss: 5.1674\n",
            "Epoch 15, Loss: 3.6164\n",
            "Epoch 16, Loss: 4.8582\n",
            "Epoch 17, Loss: 3.8594\n",
            "Epoch 18, Loss: 3.0023\n",
            "Epoch 19, Loss: 2.5497\n",
            "Epoch 20, Loss: 2.2677\n",
            "Epoch 21, Loss: 1.7869\n",
            "Epoch 22, Loss: 3.2531\n",
            "Epoch 23, Loss: 4.1885\n",
            "Epoch 24, Loss: 7.8143\n",
            "Epoch 25, Loss: 10.1721\n",
            "Epoch 26, Loss: 13.6266\n",
            "Epoch 27, Loss: 14.6170\n",
            "Epoch 28, Loss: 12.0695\n",
            "Epoch 29, Loss: 11.2052\n",
            "Epoch 30, Loss: 7.7528\n",
            "Epoch 31, Loss: 6.3641\n",
            "Epoch 32, Loss: 5.5414\n",
            "Epoch 33, Loss: 5.4329\n",
            "Epoch 34, Loss: 5.9932\n",
            "Epoch 35, Loss: 4.2969\n",
            "Epoch 36, Loss: 3.2927\n",
            "Epoch 37, Loss: 2.5046\n",
            "Epoch 38, Loss: 1.5655\n",
            "Epoch 39, Loss: 2.6855\n",
            "Epoch 40, Loss: 1.7061\n",
            "Epoch 41, Loss: 1.3156\n",
            "Epoch 42, Loss: 1.1453\n",
            "Epoch 43, Loss: 0.9160\n",
            "Epoch 44, Loss: 2.3828\n",
            "Epoch 45, Loss: 2.7095\n",
            "Epoch 46, Loss: 2.3219\n",
            "Epoch 47, Loss: 1.3808\n",
            "Epoch 48, Loss: 0.9087\n",
            "Epoch 49, Loss: 1.1188\n",
            "Epoch 50, Loss: 0.5219\n",
            "Epoch 51, Loss: 0.6518\n",
            "Epoch 52, Loss: 0.3558\n",
            "Epoch 53, Loss: 0.2178\n",
            "Epoch 54, Loss: 0.0549\n",
            "Epoch 55, Loss: 0.0415\n",
            "Epoch 56, Loss: 0.0340\n",
            "Epoch 57, Loss: 0.0320\n",
            "Epoch 58, Loss: 0.0282\n",
            "Epoch 59, Loss: 0.0270\n",
            "Epoch 60, Loss: 0.0260\n",
            "Epoch 61, Loss: 0.0229\n",
            "Epoch 62, Loss: 0.0232\n",
            "Epoch 63, Loss: 0.0213\n",
            "Epoch 64, Loss: 0.0198\n",
            "Epoch 65, Loss: 0.0194\n",
            "Epoch 66, Loss: 0.0187\n",
            "Epoch 67, Loss: 0.0179\n",
            "Epoch 68, Loss: 0.0169\n",
            "Epoch 69, Loss: 0.0172\n",
            "Epoch 70, Loss: 0.0156\n",
            "Epoch 71, Loss: 0.0170\n",
            "Epoch 72, Loss: 0.0148\n",
            "Epoch 73, Loss: 0.0148\n",
            "Epoch 74, Loss: 0.0138\n",
            "Epoch 75, Loss: 0.0140\n",
            "Epoch 76, Loss: 0.0139\n",
            "Epoch 77, Loss: 0.0132\n",
            "Epoch 78, Loss: 0.0124\n",
            "Epoch 79, Loss: 0.0123\n",
            "Epoch 80, Loss: 0.0116\n",
            "Epoch 81, Loss: 0.0112\n",
            "Epoch 82, Loss: 0.0112\n",
            "Epoch 83, Loss: 0.0108\n",
            "Epoch 84, Loss: 0.0106\n",
            "Epoch 85, Loss: 0.0107\n",
            "Epoch 86, Loss: 0.0110\n",
            "Epoch 87, Loss: 0.0108\n",
            "Epoch 88, Loss: 0.0098\n",
            "Epoch 89, Loss: 0.0093\n",
            "Epoch 90, Loss: 0.0103\n",
            "Epoch 91, Loss: 0.0099\n",
            "Epoch 92, Loss: 0.0094\n",
            "Epoch 93, Loss: 0.0086\n",
            "Epoch 94, Loss: 0.0092\n",
            "Epoch 95, Loss: 0.0084\n",
            "Epoch 96, Loss: 0.0085\n",
            "Epoch 97, Loss: 0.0081\n",
            "Epoch 98, Loss: 0.0081\n",
            "Epoch 99, Loss: 0.0076\n",
            "Epoch 100, Loss: 0.0080\n",
            "Epoch 101, Loss: 0.0075\n",
            "Epoch 102, Loss: 0.0080\n",
            "Epoch 103, Loss: 0.0073\n",
            "Epoch 104, Loss: 0.0070\n",
            "Epoch 105, Loss: 0.0070\n",
            "Epoch 106, Loss: 0.0067\n",
            "Epoch 107, Loss: 0.0068\n",
            "Epoch 108, Loss: 0.0068\n",
            "Epoch 109, Loss: 0.0066\n",
            "Epoch 110, Loss: 0.0069\n",
            "Epoch 111, Loss: 0.0062\n",
            "Epoch 112, Loss: 0.0062\n",
            "Epoch 113, Loss: 0.0066\n",
            "Epoch 114, Loss: 0.0063\n",
            "Epoch 115, Loss: 0.0060\n",
            "Epoch 116, Loss: 0.0058\n",
            "Epoch 117, Loss: 0.0057\n",
            "Epoch 118, Loss: 0.0060\n",
            "Epoch 119, Loss: 0.0058\n",
            "Epoch 120, Loss: 0.0054\n",
            "Epoch 121, Loss: 0.0055\n",
            "Epoch 122, Loss: 0.0054\n",
            "Epoch 123, Loss: 0.0053\n",
            "Epoch 124, Loss: 0.0051\n",
            "Epoch 125, Loss: 0.0051\n",
            "Epoch 126, Loss: 0.0050\n",
            "Epoch 127, Loss: 0.0049\n",
            "Epoch 128, Loss: 0.0047\n",
            "Epoch 129, Loss: 0.0049\n",
            "Epoch 130, Loss: 0.0050\n",
            "Epoch 131, Loss: 0.0048\n",
            "Epoch 132, Loss: 0.0045\n",
            "Epoch 133, Loss: 0.0048\n",
            "Epoch 134, Loss: 0.0044\n",
            "Epoch 135, Loss: 0.0044\n",
            "Epoch 136, Loss: 0.0042\n",
            "Epoch 137, Loss: 0.0044\n",
            "Epoch 138, Loss: 0.0041\n",
            "Epoch 139, Loss: 0.0040\n",
            "Epoch 140, Loss: 0.0042\n",
            "Epoch 141, Loss: 0.0040\n",
            "Epoch 142, Loss: 0.0041\n",
            "Epoch 143, Loss: 0.0039\n",
            "Epoch 144, Loss: 0.0038\n",
            "Epoch 145, Loss: 0.0037\n",
            "Epoch 146, Loss: 0.0038\n",
            "Epoch 147, Loss: 0.0038\n",
            "Epoch 148, Loss: 0.0037\n",
            "Epoch 149, Loss: 0.0037\n",
            "Epoch 150, Loss: 0.0036\n",
            "Epoch 151, Loss: 0.0036\n",
            "Epoch 152, Loss: 0.0034\n",
            "Epoch 153, Loss: 0.0034\n",
            "Epoch 154, Loss: 0.0034\n",
            "Epoch 155, Loss: 0.0034\n",
            "Epoch 156, Loss: 0.0034\n",
            "Epoch 157, Loss: 0.0033\n",
            "Epoch 158, Loss: 0.0032\n",
            "Epoch 159, Loss: 0.0032\n",
            "Epoch 160, Loss: 0.0032\n",
            "Epoch 161, Loss: 0.0034\n",
            "Epoch 162, Loss: 0.0032\n",
            "Epoch 163, Loss: 0.0030\n",
            "Epoch 164, Loss: 0.0030\n",
            "Epoch 165, Loss: 0.0030\n",
            "Epoch 166, Loss: 0.0030\n",
            "Epoch 167, Loss: 0.0030\n",
            "Epoch 168, Loss: 0.0029\n",
            "Epoch 169, Loss: 0.0028\n",
            "Epoch 170, Loss: 0.0027\n",
            "Epoch 171, Loss: 0.0028\n",
            "Epoch 172, Loss: 0.0028\n",
            "Epoch 173, Loss: 0.0027\n",
            "Epoch 174, Loss: 0.0026\n",
            "Epoch 175, Loss: 0.0026\n",
            "Epoch 176, Loss: 0.0026\n",
            "Epoch 177, Loss: 0.0025\n",
            "Epoch 178, Loss: 0.0026\n",
            "Epoch 179, Loss: 0.0025\n",
            "Epoch 180, Loss: 0.0025\n",
            "Epoch 181, Loss: 0.0024\n",
            "Epoch 182, Loss: 0.0025\n",
            "Epoch 183, Loss: 0.0024\n",
            "Epoch 184, Loss: 0.0024\n",
            "Epoch 185, Loss: 0.0024\n",
            "Epoch 186, Loss: 0.0022\n",
            "Epoch 187, Loss: 0.0023\n",
            "Epoch 188, Loss: 0.0024\n",
            "Epoch 189, Loss: 0.0024\n",
            "Epoch 190, Loss: 0.0022\n",
            "Epoch 191, Loss: 0.0022\n",
            "Epoch 192, Loss: 0.0022\n",
            "Epoch 193, Loss: 0.0021\n",
            "Epoch 194, Loss: 0.0021\n",
            "Epoch 195, Loss: 0.0022\n",
            "Epoch 196, Loss: 0.0021\n",
            "Epoch 197, Loss: 0.0021\n",
            "Epoch 198, Loss: 0.0020\n",
            "Epoch 199, Loss: 0.0020\n",
            "Epoch 200, Loss: 0.0020\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 5. Prediction\n",
        "\n",
        "Top-K sampling limits the number of possible word choices to the top K most likely ones, preventing low-quality predictions. Temperature controls randomness—higher values make the model more creative, lower values make it more predictable.\n",
        "\n",
        "\n",
        "Imagine picking ice cream flavors. With Top-K, instead of choosing from all flavors, you only pick from the top 5 best ones. Temperature is like how adventurous you are—if it's low, you always choose vanilla; if it's high, you might try a crazy new flavor.\n",
        "\n",
        "Beam search expands multiple possible sequences at each step, keeping the most likely ones. Instead of picking the best word at each step, it looks at different possibilities and selects the best overall sentence.\n",
        "\n",
        "\n",
        "Imagine you’re solving a maze, and you can explore multiple paths at once. Instead of choosing the first way that looks good, you try a few paths and pick the best one.\n",
        "\n"
      ],
      "metadata": {
        "id": "EX9OQme6_8BE"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_cbKtjZS_5Lf"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "import torch.nn.functional as F\n",
        "\n",
        "def beam_search_predict(sentence, max_words=10, temperature=1.2, top_k=5):\n",
        "    model.eval()\n",
        "\n",
        "    # Convert sentence to token indices\n",
        "    tokens = [vocab[token] if token in vocab else vocab[\"<unk>\"] for token in tokenizer(sentence)]\n",
        "    x = torch.tensor(tokens, dtype=torch.long).unsqueeze(0).to(device)\n",
        "\n",
        "    #print(f\"🔍 Input Tokens: {tokens}\")  # Debugging\n",
        "    #print(f\"🔍 Initial Tensor Shape: {x.shape}\")  # Debugging\n",
        "\n",
        "    generated_tokens = set()  # To track repeated tokens\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for _ in range(max_words):\n",
        "            output = model(x)  # Get model predictions\n",
        "            logits = output[:, -1, :]  # Take last word's logits\n",
        "\n",
        "            # **Apply temperature scaling**\n",
        "            logits = logits / temperature\n",
        "\n",
        "            # **Apply Top-k Sampling**\n",
        "            probs = F.softmax(logits, dim=-1)\n",
        "            top_k_probs, top_k_indices = torch.topk(probs, top_k, dim=-1)\n",
        "            sampled_index = torch.multinomial(top_k_probs, 1).item()\n",
        "            output_token = top_k_indices[0, sampled_index].item()\n",
        "\n",
        "            # Debugging: Show predicted token\n",
        "            #print(f\"🔍 Predicted Token Index: {output_token}, Token: {vocab.lookup_token(output_token)}\")\n",
        "\n",
        "            # Prevent infinite loop: Stop if same token is repeated too much\n",
        "            if output_token in generated_tokens:\n",
        "                #print(f\"⚠️ Stopping early: Model is repeating token '{vocab.lookup_token(output_token)}'\")\n",
        "                break\n",
        "            generated_tokens.add(output_token)\n",
        "\n",
        "            # Append the new token\n",
        "            x = torch.cat([x, torch.tensor([[output_token]], dtype=torch.long).to(device)], dim=1)\n",
        "\n",
        "    #print(f\"🔍 Final Predicted Sequence Indices: {x.tolist()}\")  # Debugging\n",
        "\n",
        "    return \" \".join([vocab.lookup_token(i) for i in x.squeeze().tolist()])\n",
        "\n",
        "# Run Beam Search Prediction\n",
        "# print(beam_search_predict(\"hello\", temperature=1.2, top_k=5))\n",
        "# print(beam_search_predict(\"attention\", temperature=1.2, top_k=5))\n",
        "# print(beam_search_predict(\"learning\", temperature=1.2, top_k=5))\n",
        "\n",
        "\n",
        "# print(beam_search_predict(\"hello\", temperature=0.8, top_k=5))\n",
        "# print(beam_search_predict(\"transformers\", temperature=0.8, top_k=5))\n",
        "# print(beam_search_predict(\"vision\", temperature=0.8, top_k=5))\n",
        "# print(beam_search_predict(\"attention\", temperature=0.8, top_k=5))\n",
        "# print(beam_search_predict(\"learning\", temperature=0.8, top_k=5))\n",
        "\n",
        "\n",
        "print(beam_search_predict(\"hello\", temperature=0.8, top_k=10))\n",
        "print(beam_search_predict(\"transformers\", temperature=0.8, top_k=10))\n",
        "print(beam_search_predict(\"vision\", temperature=0.8, top_k=10))\n",
        "print(beam_search_predict(\"attention\", temperature=0.8, top_k=10))\n",
        "print(beam_search_predict(\"learning\", temperature=0.8, top_k=10))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L_uJiZcjY5D6",
        "outputId": "f1f9d77d-1898-4db5-a285-b3da295d3e1b"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<unk> process is\n",
            "transformers for the\n",
            "vision captures is\n",
            "attention captures sequences\n",
            "learning learning\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Enhance prediction"
      ],
      "metadata": {
        "id": "myNMdn_PANH3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def beam_search_predict(sentence, max_words=8, temperature=0.8, top_k=5, top_p=0.85, penalty=1.5, beam_width=3, length_penalty=1.0):\n",
        "    model.eval()\n",
        "\n",
        "    # Convert sentence to token indices\n",
        "    tokens = [vocab[token] if token in vocab else vocab[\"<unk>\"] for token in tokenizer(sentence)]\n",
        "    x = torch.tensor(tokens, dtype=torch.long).unsqueeze(0).to(device)\n",
        "\n",
        "    generated_tokens = set()  # Track repeated tokens\n",
        "    sequences = [(x, 0)]  # Store (sequence, score)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for _ in range(max_words):\n",
        "            all_candidates = []\n",
        "            for seq, score in sequences:\n",
        "                output = model(seq)\n",
        "\n",
        "                # Apply temperature scaling\n",
        "                logits = output[:, -1, :] / temperature\n",
        "                probs = F.softmax(logits, dim=-1)\n",
        "\n",
        "                # Apply top-k and top-p filtering\n",
        "                sorted_probs, sorted_indices = torch.sort(probs, descending=True)\n",
        "                cumulative_probs = torch.cumsum(sorted_probs, dim=-1)\n",
        "\n",
        "                # Apply top-p filtering (keep the most probable tokens that sum to top_p)\n",
        "                top_p_mask = cumulative_probs <= top_p\n",
        "                top_p_mask[:, 1:] = top_p_mask[:, :-1].clone()\n",
        "                top_p_mask[:, 0] = True  # Always keep at least one token\n",
        "\n",
        "                # Get valid token indices\n",
        "                valid_indices = sorted_indices[top_p_mask]\n",
        "                valid_probs = sorted_probs[top_p_mask]\n",
        "\n",
        "                # Sample from filtered distribution\n",
        "                sampled_index = valid_indices[torch.multinomial(valid_probs, 1)].item()\n",
        "\n",
        "                # Apply repetition penalty\n",
        "                if sampled_index in generated_tokens:\n",
        "                    probs[:, sampled_index] /= penalty\n",
        "\n",
        "                # Append the new token\n",
        "                new_seq = torch.cat([seq, torch.tensor([[sampled_index]], dtype=torch.long).to(device)], dim=1)\n",
        "                new_score = score - torch.log(probs[:, sampled_index]) / (len(new_seq) ** length_penalty)  # Normalize score\n",
        "\n",
        "                all_candidates.append((new_seq, new_score))\n",
        "\n",
        "            # Keep best `beam_width` sequences\n",
        "            sequences = sorted(all_candidates, key=lambda x: x[1])[:beam_width]\n",
        "\n",
        "            # Stop early if all beams generate `<unk>` or repeated tokens\n",
        "            if all(seq[0][:, -1].item() == vocab[\"<unk>\"] or seq[0][:, -1].item() in generated_tokens for seq in sequences):\n",
        "                break\n",
        "\n",
        "            # Track generated tokens\n",
        "            generated_tokens.update([seq[0][:, -1].item() for seq in sequences])\n",
        "\n",
        "    # Choose the best sequence\n",
        "    best_sequence = sequences[0][0].squeeze().tolist()\n",
        "    return \" \".join([vocab.lookup_token(i) for i in best_sequence])\n",
        "\n",
        "# 🔥 Test the improved function\n",
        "print(beam_search_predict(\"hello\", temperature=0.8, top_k=5))\n",
        "print(beam_search_predict(\"transformers\", temperature=0.8, top_k=5))\n",
        "print(beam_search_predict(\"vision\", temperature=0.8, top_k=5))\n",
        "print(beam_search_predict(\"attention\", temperature=0.8, top_k=5))\n",
        "print(beam_search_predict(\"learning\", temperature=0.8, top_k=5))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "efMju7Hth6rt",
        "outputId": "cd0ec96c-37c4-4551-a938-a083d7023a3f"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<unk> process is is\n",
            "transformers for for\n",
            "vision captures transformers is from on on\n",
            "attention captures sequences sequences\n",
            "learning learning learning\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def beam_search_predict(sentence, max_words=6, temperature=0.8, top_k=5, top_p=0.85, penalty=2.0, beam_width=5, diversity_penalty=1.2):\n",
        "    model.eval()\n",
        "\n",
        "    # Convert sentence to token indices\n",
        "    tokens = [vocab[token] if token in vocab else vocab[\"<unk>\"] for token in tokenizer(sentence)]\n",
        "    x = torch.tensor(tokens, dtype=torch.long).unsqueeze(0).to(device)\n",
        "\n",
        "    generated_tokens = set()  # Track repeated tokens\n",
        "    sequences = [(x, 0)]  # Store (sequence, score)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for _ in range(max_words):\n",
        "            all_candidates = []\n",
        "            for seq, score in sequences:\n",
        "                output = model(seq)\n",
        "\n",
        "                # Apply temperature scaling\n",
        "                logits = output[:, -1, :] / temperature\n",
        "                probs = F.softmax(logits, dim=-1)\n",
        "\n",
        "                # Apply top-k and top-p filtering\n",
        "                sorted_probs, sorted_indices = torch.sort(probs, descending=True)\n",
        "                cumulative_probs = torch.cumsum(sorted_probs, dim=-1)\n",
        "\n",
        "                # Apply top-p filtering\n",
        "                top_p_mask = cumulative_probs <= top_p\n",
        "                top_p_mask[:, 1:] = top_p_mask[:, :-1].clone()\n",
        "                top_p_mask[:, 0] = True  # Always keep at least one token\n",
        "\n",
        "                # Get valid token indices\n",
        "                valid_indices = sorted_indices[top_p_mask]\n",
        "                valid_probs = sorted_probs[top_p_mask]\n",
        "\n",
        "                # Sample from filtered distribution\n",
        "                sampled_index = valid_indices[torch.multinomial(valid_probs, 1)].item()\n",
        "\n",
        "                # Apply repetition penalty\n",
        "                if sampled_index in generated_tokens:\n",
        "                    probs[:, sampled_index] /= penalty\n",
        "\n",
        "                # Apply diversity penalty (promotes different outputs)\n",
        "                probs /= (diversity_penalty ** len(seq))\n",
        "\n",
        "                # Append the new token\n",
        "                new_seq = torch.cat([seq, torch.tensor([[sampled_index]], dtype=torch.long).to(device)], dim=1)\n",
        "                new_score = score - torch.log(probs[:, sampled_index])  # Normalize score\n",
        "\n",
        "                all_candidates.append((new_seq, new_score))\n",
        "\n",
        "            # Keep best `beam_width` sequences\n",
        "            sequences = sorted(all_candidates, key=lambda x: x[1])[:beam_width]\n",
        "\n",
        "            # Stop early if all beams generate `<unk>` or repeated tokens\n",
        "            if all(seq[0][:, -1].item() == vocab[\"<unk>\"] or seq[0][:, -1].item() in generated_tokens for seq in sequences):\n",
        "                break\n",
        "\n",
        "            # Track generated tokens\n",
        "            generated_tokens.update([seq[0][:, -1].item() for seq in sequences])\n",
        "\n",
        "    # Choose the best sequence\n",
        "    best_sequence = sequences[0][0].squeeze().tolist()\n",
        "    return \" \".join([vocab.lookup_token(i) for i in best_sequence])\n",
        "\n",
        "# 🔥 Test the improved function\n",
        "print(beam_search_predict(\"hello\", temperature=0.8, top_k=5))\n",
        "print(beam_search_predict(\"transformers\", temperature=0.8, top_k=5))\n",
        "print(beam_search_predict(\"vision\", temperature=0.8, top_k=5))\n",
        "print(beam_search_predict(\"attention\", temperature=0.8, top_k=5))\n",
        "print(beam_search_predict(\"learning\", temperature=0.8, top_k=5))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xe6WdBNMiYe6",
        "outputId": "b69ce3ed-1184-42f7-a4d0-a40fcd852139"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<unk> process is is\n",
            "transformers for the for\n",
            "vision captures captures\n",
            "attention captures sequences sequences\n",
            "learning learning learning\n"
          ]
        }
      ]
    }
  ]
}